{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8dd7cb",
   "metadata": {},
   "source": [
    "#  1. Load  and  Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04e85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USE_CATEGORICAL = False\n",
    "\n",
    "TRITON_IMAGE = 'nvcr.io/nvidia/tritonserver:21.12-py3'\n",
    "\n",
    "!docker pull {TRITON_IMAGE}\n",
    "\n",
    "#!kaggle competitions download -c ieee-fraud-detection\n",
    "#!unzip -u ieee-fraud-detection.zip\n",
    "train_csv = 'train_transaction.csv'\n",
    "\n",
    "!export NUMBA_CUDA_ENABLE_MINOR_VERSION_COMPATIBILITY=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529fe1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer\n",
    "if not USE_CATEGORICAL:\n",
    "    from cuml.preprocessing import LabelEncoder\n",
    "# Due to an upstream bug, cuML's train_test_split function is\n",
    "# currently non-deterministic. We will therefore use sklearn's\n",
    "# train_test_split in this example to obtain more consistent\n",
    "# results.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b440e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV files into cuDF DataFrames\n",
    "data = cudf.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de552313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='median')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456211a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert string columns to categorical or perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "if USE_CATEGORICAL:\n",
    "    data[cat_columns.columns] = cat_columns.astype('category')\n",
    "else:\n",
    "    for col in cat_columns.columns:\n",
    "        data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720aacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3d8cc",
   "metadata": {},
   "source": [
    "#  2. Build  XGBoost  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc5afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec11f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model training function\n",
    "def train_model(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        enable_categorical=USE_CATEGORICAL,\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc239af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train a small model with just 500 trees and a maximum depth of 3\n",
    "small_model = train_model(500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57524f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train a large model with 5000 trees and a maximum depth of 12\n",
    "large_model = train_model(5000, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8ab0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33ec4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "def serialize_model(model, model_name):\n",
    "    # The name of the model directory determines the name of the model as reported\n",
    "    # by Triton\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    # We can store multiple versions of the model in the same directory. In our\n",
    "    # case, we have just one version, so we will add a single directory, named '1'.\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "    # It is recommended that you use this filename to avoid having to specify a\n",
    "    # name in the configuration file.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir\n",
    "\n",
    "small_model_dir = serialize_model(small_model, 'small_model')\n",
    "small_model_cpu_dir = serialize_model(small_model, 'small_model-cpu')\n",
    "large_model_dir = serialize_model(large_model, 'large_model')\n",
    "large_model_cpu_dir = serialize_model(large_model, 'large_model-cpu')\n",
    "\n",
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes. This can be increased using the\n",
    "# `--cuda-memory-pool-byte-size` option when the server is\n",
    "# started, but this notebook should work fine with default\n",
    "# settings.\n",
    "MAX_MEMORY_BYTES = 60_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43016177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "def generate_config(model_dir, deployment_type='gpu', storage_type='AUTO'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "\n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path\n",
    "\n",
    "generate_config(small_model_dir, deployment_type='gpu')\n",
    "generate_config(small_model_cpu_dir, deployment_type='cpu')\n",
    "generate_config(large_model_dir, deployment_type='gpu')\n",
    "generate_config(large_model_cpu_dir, deployment_type='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32073b",
   "metadata": {},
   "source": [
    "# 3. Start Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab588d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdea35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb97f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run --gpus all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e2f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run --gpus device=0 -d -p 8000:8000 -p 8001:8001 -p 8002:8002 -v {REPO_PATH}:/models --name tritonserver {TRITON_IMAGE} tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9196",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = 'localhost'\n",
    "PORT = 8001\n",
    "TIMEOUT = 60\n",
    "\n",
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')\n",
    "\n",
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "\n",
    "!docker logs tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36163d96",
   "metadata": {},
   "source": [
    "# 4. Compare runs local vs  Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651c3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fdc91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160d39e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec20f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f7f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    triton_result = triton_predict('large_model', np_data[0:5])\n",
    "#print(\"Result computed on Triton: \")\n",
    "#print(triton_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72560071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    large_model.predict_proba(X_test[0:10000000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e77260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_result = large_model.predict_proba(X_test[0:1000000000])\n",
    "print(\"\\nResult computed locally: \")\n",
    "print(local_result)\n",
    "#cp.testing.assert_allclose(triton_result, local_result, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7bc7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cp.testing.assert_allclose(triton_result, local_result, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c654c",
   "metadata": {},
   "source": [
    "# 5. Fine tuning  batch size and concurrent batches  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --net=host -v $(pwd):/workspace/host nvcr.io/nvidia/tritonserver:23.02-py3-sdk perf_analyzer -m large_model -b 50 --concurrency-range 2:20:2  > log_b50_c2_20_2               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8cf37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
